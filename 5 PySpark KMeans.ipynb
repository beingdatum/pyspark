{"cells":[{"cell_type":"markdown","source":["Welcome to exercise two of week three of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise we’ll work on clustering.\n\nLet’s create our DataFrame again:"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["%sh\n\n# delete files from previous runs\n#rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n#wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n#ls -ltr /databricks/driver\n#mkdir /dbfs/tmp/HMPPARQ/\n#cp /databricks/driver/hmp.parquet /dbfs/tmp/HMPPARQ/\nls -ltr /dbfs/tmp/HMPPARQ/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 0\n-rw-r--r-- 1 root root 932997 Jun 25 12:08 hmp.parquet\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# create a dataframe out of it\ndf = spark.read.parquet('/tmp/HMPPARQ/hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+\n  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|\n 21| 51| 33|Accelerometer-201...|Brush_teeth|\n 20| 50| 34|Accelerometer-201...|Brush_teeth|\n 21| 49| 33|Accelerometer-201...|Brush_teeth|\n 21| 49| 33|Accelerometer-201...|Brush_teeth|\n 20| 51| 35|Accelerometer-201...|Brush_teeth|\n 18| 49| 34|Accelerometer-201...|Brush_teeth|\n 19| 48| 34|Accelerometer-201...|Brush_teeth|\n 16| 53| 34|Accelerometer-201...|Brush_teeth|\n 18| 52| 35|Accelerometer-201...|Brush_teeth|\n 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Let’s reuse our feature engineering pipeline."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\nencoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\nprediction.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Now let’s create a new pipeline for kmeans."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nkmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Silhouette with squared euclidean distance = 0.41244594513295846\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["We have 14 different movement patterns in the dataset, so setting K of KMeans to 14 is a good idea. But please experiment with different values for K, do you find a sweet spot? The closer Silhouette gets to 1, the better.\n\nhttps://en.wikipedia.org/wiki/Silhouette_(clustering)"],"metadata":{}},{"cell_type":"code","source":["# please change the pipeline the check performance for different K, feel free to use a loop"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now please extend the pipeline to work on the normalized features. You need to tell KMeans to use the normalized feature column and change the pipeline in order to contain the normalizer stage as well."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3.6 with Spark","language":"python3","name":"python36"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"name":"6 PySpark KMeans","notebookId":3833656855695103},"nbformat":4,"nbformat_minor":0}
