{"cells":[{"cell_type":"markdown","source":["# Dataframes"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["* Dataframes are a restricted sub-type of RDDs. \n* Restircing the type allows for more optimization."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* Dataframes store two dimensional data, similar to the type of data stored in a spreadsheet. \n   * Each column in a dataframe can have a different type.\n   * Each row contains a `record`."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["* Similar to, but not the same as, [pandas dataframes](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) and [R dataframes](http://www.r-tutor.com/r-introduction/data-frame)"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"code","source":["import os\nimport sys\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Constructing a DataFrame from an RDD of Rows\nEach Row defines it's own  fields, the schema is *inferred*."],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["# One way to create a DataFrame is to first define an RDD from a list of Rows \nsome_rdd = sc.parallelize([Row(name=u\"John\", age=19),\n                           Row(name=u\"Smith\", age=23),\n                           Row(name=u\"Sarah\", age=18)])\nsome_rdd.collect()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [Row(age=19, name=&#39;John&#39;),\n Row(age=23, name=&#39;Smith&#39;),\n Row(age=18, name=&#39;Sarah&#39;)]</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# The DataFrame is created from the RDD or Rows\n# Infer schema from the first row, create a DataFrame and print the schema\nsome_df = spark.createDataFrame(some_rdd)\nsome_df.printSchema()\nsome_df.show()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- age: long (nullable = true)\n-- name: string (nullable = true)\n\n+---+-----+\nage| name|\n+---+-----+\n 19| John|\n 23|Smith|\n 18|Sarah|\n+---+-----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# A dataframe is an RDD of rows plus information on the schema.\n# performing **collect()* on either the RDD or the DataFrame gives the same result.\nprint(type(some_rdd),type(some_df))\nprint('some_df =',some_df.collect())\nprint('some_rdd=',some_rdd.collect())\nsome_df.show()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt; &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\nsome_df = [Row(age=19, name=&#39;John&#39;), Row(age=23, name=&#39;Smith&#39;), Row(age=18, name=&#39;Sarah&#39;)]\nsome_rdd= [Row(age=19, name=&#39;John&#39;), Row(age=23, name=&#39;Smith&#39;), Row(age=18, name=&#39;Sarah&#39;)]\n+---+-----+\nage| name|\n+---+-----+\n 19| John|\n 23|Smith|\n 18|Sarah|\n+---+-----+\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### Defining the Schema explicitly\nThe advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows."],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\nanother_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n# Schema with two fields - person_name and person_age\nschema = StructType([StructField(\"person_name\", StringType(), False),\n                     StructField(\"person_age\", IntegerType(), False)])\n\n# Create a DataFrame by applying the schema to the RDD and print the schema\nanother_df = spark.createDataFrame(another_rdd, schema)\nanother_df.printSchema()\nanother_df.show()\n# root\n#  |-- age: binteger (nullable = true)\n#  |-- name: string (nullable = true)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- person_name: string (nullable = false)\n-- person_age: integer (nullable = false)\n\n+-----------+----------+\nperson_name|person_age|\n+-----------+----------+\n       John|        19|\n      Smith|        23|\n      Sarah|        18|\n+-----------+----------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Loading DataFrames from disk\nThere are many maethods to load DataFrames from Disk. Here we will discuss three of these methods\n1. Parquet\n2. JSON (on your own)\n3. CSV  (on your own)\n\nIn addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class."],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["### Loading dataframes from JSON files\n[JSON](http://www.json.org/) is a very popular readable file format for storing structured data.\nAmong it's many uses are **twitter**, `javascript` communication packets, and many others. In fact this notebook file (with the extension `.ipynb` is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe."],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"code","source":["!wget 'https://mas-dse-open.s3.amazonaws.com/Moby-Dick.txt' -P ../../Data/"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# when loading json files you can specify either a single file or a directory containing many json files.\npath = \"/FileStore/tables/people.json\"\n\n# Create a DataFrame from the file(s) pointed to by path\npeople = spark.read.json(path)\nprint('people is a',type(people))\n# The inferred schema can be visualized using the printSchema() method.\npeople.show()\n\npeople.printSchema()"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">people is a &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+----+-------+\n age|   name|\n+----+-------+\nnull|Michael|\n  30|   Andy|\n  19| Justin|\n+----+-------+\n\nroot\n-- age: long (nullable = true)\n-- name: string (nullable = true)\n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### Excercise: Loading csv files into dataframes\n\nSpark 2.0 includes a facility for reading csv files. In this excercise you are to create similar functionality using your own code.\n\nYou are to write a class called `csv_reader` which has the following methods:\n\n* `__init__(self,filepath):` recieves as input the path to a csv file. It throws an exeption `NoSuchFile` if the file does not exist.\n* `Infer_Schema()` opens the file, reads the first 10 lines (or less if the file is shorter), and infers the schema. The first line of the csv file defines the column names. The following lines should have the same number of columns and all of the elements of the column should be of the same type. The only types allowd are `int`,`float`,`string`. The method infers the types of the columns, checks that they are consistent, and defines a dataframe schema of the form:\n```python\nschema = StructType([StructField(\"person_name\", StringType(), False),\n                     StructField(\"person_age\", IntegerType(), False)])\n```\nIf everything checks out, the method defines a `self.` variable that stores the schema and returns the schema as it's output. If an error is found an exception `BadCsvFormat` is raised.\n* `read_DataFrame()`: reads the file, parses it and creates a dataframe using the inferred schema. If one of the lines beyond the first 10 (i.e. a line that was not read by `InferSchema`) is not parsed correctly, the line is not added to the Dataframe. Instead, it is added to an RDD called `bad_lines`.\nThe methods returns the dateFrame and the `bad_lines` RDD."],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"markdown","source":["### Parquet files"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["* [Parquet](http://parquet.apache.org/) is a popular columnar format."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* Spark SQL allows [SQL](https://en.wikipedia.org/wiki/SQL) queries to retrieve a subset of the rows without reading the whole file."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* Compatible with HDFS : allows parallel retrieval on a cluster."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* Parquet compresses the data in each column."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["### Spark and Hive\n* Parquet is a **file format** not an independent database server.\n* Spark can work with the [Hive](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started) relational database system that supports the full array of database operations.\n* Hive is compatible with HDFS."],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"code","source":["parquet_file=\"/FileStore/tables/users.parquet\""],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["#load a Parquet file\nprint(parquet_file)\ndf = spark.read.load(parquet_file)\ndf.show()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/FileStore/tables/users.parquet\n+------+--------------+----------------+\n  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\nAlyssa|          null|  [3, 9, 15, 20]|\n   Ben|           red|              []|\n+------+--------------+----------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["df2=df.select(\"name\", \"favorite_color\")\ndf2.show()"],"metadata":{"scrolled":true,"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+--------------+\n  name|favorite_color|\n+------+--------------+\nAlyssa|          null|\n   Ben|           red|\n+------+--------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["outfilename=\"namesAndFavColors.parquet\"\ndf2.write.save(\"/\"+outfilename)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["A new interface object has been added in **Spark 2.0** called **SparkSession**. A spark session is initialized using a `builder`. For example\n```python\nspark = SparkSession.builder \\\n         .master(\"local\") \\\n         .appName(\"Word Count\") \\\n         .config(\"spark.some.config.option\", \"some-value\") \\\n         .getOrCreate()\n```\n\nUsing a SparkSession a Parquet file is read [as follows:](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet):\n```python\ndf = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n```"],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"markdown","source":["## Summary\n* Dataframes are an efficient way to store data tables.\n* All of the values in a column have the same type.\n* A good way to store a dataframe in disk is to use a Parquet file.\n* Next: Operations on dataframes."],"metadata":{"slideshow":{"slide_type":"subslide"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"3 DF & SparkSQL","notebookId":1923096657762430,"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{},"toc":{"title_sidebar":"Contents","nav_menu":{"height":"263px","width":"252px"},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false,"title_cell":"Table of Contents"},"celltoolbar":"Slideshow","varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"nbformat":4,"nbformat_minor":0}
