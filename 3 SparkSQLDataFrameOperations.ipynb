{"cells":[{"cell_type":"code","source":["%sh\nwget https://mas-dse-open.s3.amazonaws.com/Weather/by_state/NY.tgz\ntar zxvf NY.tgz\n#ls -ltr /databricks/driver/NY.parquet/\ncp /databricks/driver/NY.parquet/* /dbfs/tmp/NYPARQ/\nls -ltr /dbfs/tmp/NYPARQ/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 0\n-rw-r--r-- 1 root root 4138689 Jun 20 11:46 part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3660544 Jun 20 11:46 part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3946287 Jun 20 11:46 part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 4004047 Jun 20 11:46 part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2836314 Jun 20 11:46 part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3949516 Jun 20 11:46 part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3317024 Jun 20 11:46 part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2616239 Jun 20 11:46 part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3929807 Jun 20 11:46 part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3865388 Jun 20 11:46 part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3655620 Jun 20 11:46 part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3333004 Jun 20 11:46 part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3532197 Jun 20 11:46 part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 1431820 Jun 20 11:46 part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3913946 Jun 20 11:46 part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2088969 Jun 20 11:46 part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2340575 Jun 20 11:46 part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 1737280 Jun 20 11:46 part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2007068 Jun 20 11:46 part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 1600288 Jun 20 11:46 part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3170504 Jun 20 11:46 part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 1766711 Jun 20 11:46 part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2507358 Jun 20 11:46 part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2094306 Jun 20 11:46 part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 1421413 Jun 20 11:46 part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 2705496 Jun 20 11:46 part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root       0 Jun 20 11:46 _SUCCESS\n-rw-r--r-- 1 root root  103149 Jun 20 11:46 part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n-rw-r--r-- 1 root root 3951224 Jun 20 11:46 part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n\ndf = spark.read.parquet(\"/tmp/NYPARQ/\")\ndf.show(1)"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\nUSW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## Dataframe operations\nSpark DataFrames allow operations similar to pandas Dataframes. We demonstrate some of those.\n\nFor more, see the [official guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) and [this article](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Station: string (nullable = true)\n-- Measurement: string (nullable = true)\n-- Year: long (nullable = true)\n-- Values: binary (nullable = true)\n-- dist_coast: double (nullable = true)\n-- latitude: double (nullable = true)\n-- longitude: double (nullable = true)\n-- elevation: double (nullable = true)\n-- state: string (nullable = true)\n-- name: string (nullable = true)\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["print(df.count())\ndf.show(1)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">168398\n+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\nUSW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### .describe()\nThe method `df.describe()` computes five statistics for each column of the dataframe `df`.\n\nThe statistics are: **count, mean, std, min,max**"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["You get the following man page using the command `df.describe?`\n\n```\nSignature: df.describe(*cols)\nDocstring:\nComputes statistics for numeric and string columns.\n\nThis include count, mean, stddev, min, and max. If no columns are\ngiven, this function computes statistics for all numerical or string columns.\n\n.. note:: This function is meant for exploratory data analysis, as we make no\n    guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n>>> df.describe(['age']).show()\n+-------+------------------+\n|summary|               age|\n+-------+------------------+\n|  count|                 2|\n|   mean|               3.5|\n| stddev|2.1213203435596424|\n|    min|                 2|\n|    max|                 5|\n+-------+------------------+\n>>> df.describe().show()\n+-------+------------------+-----+\n|summary|               age| name|\n+-------+------------------+-----+\n|  count|                 2|    2|\n|   mean|               3.5| null|\n| stddev|2.1213203435596424| null|\n|    min|                 2|Alice|\n|    max|                 5|  Bob|\n+-------+------------------+-----+\n\n.. versionadded:: 1.3.1\nFile:      ~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\nType:      method\n```"],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"code","source":["df.describe().select('station','measurement').show()"],"metadata":{"scrolled":false,"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+\n    station|measurement|\n+-----------+-----------+\n     168398|     168398|\n       null|       null|\n       null|       null|\nUSC00300015|       PRCP|\nUSW00094794|   TOBS_s20|\n+-----------+-----------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["#### groupby and agg\nThe method `.groupby(col)` groups rows according the value of the column `col`.  \nThe method `.agg(spec)` computes a summary for each group as specified in `spec`"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["df.groupby('measurement').agg({'year': 'min', 'station':'count'}).show()"],"metadata":{"scrolled":true,"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---------+--------------+\nmeasurement|min(year)|count(station)|\n+-----------+---------+--------------+\n   TMIN_s20|     1873|         13442|\n       TMIN|     1873|         13442|\n   SNOW_s20|     1884|         15629|\n       TOBS|     1876|         10956|\n   SNWD_s20|     1888|         14617|\n   PRCP_s20|     1871|         16118|\n   TOBS_s20|     1876|         10956|\n       TMAX|     1873|         13437|\n       SNOW|     1884|         15629|\n   TMAX_s20|     1873|         13437|\n       SNWD|     1888|         14617|\n       PRCP|     1871|         16118|\n+-----------+---------+--------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# THis command will load the python module that defines the SQL functions\n#%load ls ~/spark-latest/python/pyspark/sql/functions.py"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Using SQL queries on DataFrames\n\nThere are two main ways to manipulate  DataFrames:"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["#### Imperative manipulation\nUsing python methods such as `.select` and `.groupby`.\n* Advantage: order of operations is specified.\n* Disrdavantage : You need to describe both **what** is the result you want and **how** to get it."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["#### Declarative Manipulation (SQL)\n* Advantage: You need to describe only **what** is the result you want.\n* Disadvantage: SQL does not have primitives for common analysis operations such as **covariance**"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["### Using sql commands on a dataframe\nSpark supports a [subset](https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features) of the Hive SQL query language.\n\nFor example, You can use [Hive `select` syntax](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select) to select a subset of the rows in a dataframe.\n\nTo use sql on a dataframe you need to first `register` it as a `TempTable`.\n\nfor variety, we are using here a small dataframe loaded from a JSON file."],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"code","source":["# when loading json files you can specify either a single file or a directory containing many json files.\npath = \"/FileStore/tables/people.json\"\n\n# Create a DataFrame from the file(s) pointed to by path\npeople = spark.read.json(path)\n#print('people is a',type(people))\n# The inferred schema can be visualized using the printSchema() method.\npeople.show()"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------+\n age|   name|\n+----+-------+\nnull|Michael|\n  30|   Andy|\n  19| Justin|\n+----+-------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["people.printSchema()"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- age: long (nullable = true)\n-- name: string (nullable = true)\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Register this DataFrame as a table.\npeople.registerTempTable(\"people\")\n\n# SQL statements can be run by using the sql methods provided by sqlContext\nteenagers = sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\nteenagers.show()\nteenagers.write.save(\"teenagersdata\")"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\n  name|\n+------+\nJustin|\n+------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["%sh\n\nls -ltr /dbfs/teenagersdata/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 0\n-rw-r--r-- 1 root root   0 Jun 21 06:12 _started_3226332471664364021\n-rw-r--r-- 1 root root 445 Jun 21 06:12 part-00000-tid-3226332471664364021-bd85ae5c-239c-454b-862e-d6aa5d2b7e20-387-1-c000.snappy.parquet\n-rw-r--r-- 1 root root   0 Jun 21 06:12 _SUCCESS\n-rw-r--r-- 1 root root 124 Jun 21 06:12 _committed_3226332471664364021\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["#### Counting the number of occurances of each measurement, imparatively"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["L=df.groupBy('measurement').count().collect()\n#L is a list (collected DataFrame)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["D=[(e.measurement,e['count']) for e in L]\nsorted(D,key=lambda x:x[1], reverse=False)[:6]"],"metadata":{"scrolled":false,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[55]: [(&#39;TOBS&#39;, 10956),\n (&#39;TOBS_s20&#39;, 10956),\n (&#39;TMAX&#39;, 13437),\n (&#39;TMAX_s20&#39;, 13437),\n (&#39;TMIN_s20&#39;, 13442),\n (&#39;TMIN&#39;, 13442)]</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["#### Counting the number of occurances of each measurement, declaratively."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["df.registerTempTable('weather') "],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["query=\"\"\"\nSELECT measurement,COUNT(measurement) AS count,\n                   MIN(year) AS MinYear \nFROM weather  \nGROUP BY measurement \nORDER BY count\n\"\"\"\nprint(query)\n\nsql(query).show()"],"metadata":{"scrolled":true,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nSELECT measurement,COUNT(measurement) AS count,\n                   MIN(year) AS MinYear \nFROM weather  \nGROUP BY measurement \nORDER BY count\n\n+-----------+-----+-------+\nmeasurement|count|MinYear|\n+-----------+-----+-------+\n   TOBS_s20|10956|   1876|\n       TOBS|10956|   1876|\n       TMAX|13437|   1873|\n   TMAX_s20|13437|   1873|\n       TMIN|13442|   1873|\n   TMIN_s20|13442|   1873|\n   SNWD_s20|14617|   1888|\n       SNWD|14617|   1888|\n       SNOW|15629|   1884|\n   SNOW_s20|15629|   1884|\n   PRCP_s20|16118|   1871|\n       PRCP|16118|   1871|\n+-----------+-----+-------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["#### Performing a map command\n* In order to perform a `map` on a dataframe, you first need to transform it into an RDD."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["* **Not** the recommended way. Better way is to use built-in sparkSQL functions.\n* Or register new ones (Advanced)."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"code","source":["def find_century(row):\n    if row.Year < 1900:\n        return '19th'\n    elif row.Year <2000:\n        return '20th'\n    elif row.Year <2010:\n        return '21st'\n    else:\n        return 'possibly_bad_data'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["df.rdd.map(find_century).take(5)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: [&#39;20th&#39;, &#39;20th&#39;, &#39;20th&#39;, &#39;20th&#39;, &#39;20th&#39;]</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["#### Aggregations \n* **Aggregation** can be used, in combination with built-in sparkSQL functions \nto compute statistics of a dataframe.\n* computation will be fast thanks to combined optimzations with database operations."],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["* A partial list : `count(), approx_count_distinct(), avg(), max(), min()`"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* Of these, the interesting one is `approx_count_distinct()` which uses sampling to get an approximate count fast."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["* [The gory details](http://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/sql/functions.html)"],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F # used here just for show."],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["df.agg({'station':'approx_count_distinct'}).show()"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------------+\napprox_count_distinct(station)|\n+------------------------------+\n                           339|\n+------------------------------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["#### Approximate Quantile\n\n* Suppose we want to partition the years into 10 ranges\n* such that in each range we have approximately the same number of records.\n* The method `.approxQuantile` will use a sample to do this for us."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["print('with accuracy 0.1: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.1))\nprint('with accuracy 0.01: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.01))"],"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">with accuracy 0.1:  [1871.0, 1928.0, 1948.0, 1957.0, 1965.0, 1974.0, 1984.0, 1994.0, 2013.0]\nwith accuracy 0.01:  [1920.0, 1938.0, 1949.0, 1957.0, 1965.0, 1975.0, 1984.0, 1993.0, 2003.0]\n</div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["#### Lets collect the exact number of rows for each year\nThis will take much longer than ApproxQuantile on a large file"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["# Lets collect the exact number of rows for each year ()\nquery='SELECT year,COUNT(year) AS count FROM weather GROUP BY year ORDER BY year'\nprint(query)\ncounts=sqlContext.sql(query)\nprint('counts is ',counts)"],"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SELECT year,COUNT(year) AS count FROM weather GROUP BY year ORDER BY year\ncounts is  DataFrame[year: bigint, count: bigint]\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["import pandas as pd    \nA=counts.toPandas() # Transform a spark Dataframe to a Pandas Dataframe\nA.plot.line('year','count')\ngrid()"],"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Summary\n\n* Dataframes can be manipulated decleratively, which allows for more optimization.\n* Dataframes can be stored and retrieved from Parquet files.\n* It is possible to refer directly to a parquet file in an SQL query.\n* See you next time!"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["## References\n* For an introduction to Spark SQL and Dataframes see: [Spark SQL, DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide)\n* Also [spark-dataframe-and-operations](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/) from [analyticsvidhya.com](https://www.analyticsvidhya.com)\n\nFor complete API reference see\n* [SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) For Java, Scala and Python (Implementation is first in Scala and Python, later pyspark)\n* [pyspark API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) \n* [pyspark API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"],"metadata":{"slideshow":{"slide_type":"skip"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"4 SparkSQLDataFrameOperations","notebookId":1923096657762467,"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{},"toc":{"title_sidebar":"Contents","nav_menu":{"height":"263px","width":"252px"},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false,"title_cell":"Table of Contents"},"celltoolbar":"Slideshow","varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
